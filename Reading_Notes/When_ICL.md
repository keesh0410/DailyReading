1. LLM + ICL （就是通过Prompt 描述任务给llm) 的方法在复杂任务指令上往往失效.

如果你要做一件事情，这件事情要求很多的时候，比如说事件标注，要标注时间但是如果这个时间十年以上就不要，标注人数，人数多少人到多少人标记为某一组 xxx 要求一堆的那种任务. 

表现都是失效.

那这个问题 你要怎么研究?

很直观的就是，我不知道怎么去研究这个问题.

今天看到一篇paper? 我不应该顺着这个思路哦哦哦做这个分析 然后做那个分析.

就是我们怎么去
substantiate (证实）和 posit (构造） 一个假设 （两个今天读论文新学到的高级词，自己多说几次）

我们读paper的时候如果只是当个读论文工具人，你读这个到底要干什么?


-----------

他先把复杂指令失效这件事情，拆解成 LLM + ICL 两部分， 到底是大模型本身就有这个缺陷，还是 ICL 的缺陷》


LLM 在子任务上面做微调之后， 就能达到类似于SOTA的水准。 
ok, 那现在说明 LLM 本身是有能力做到的. 

那接下来就是ICL，为什么ICL失效了。

提出思考假设： 应该是1. 机器没办法真正的理解我的语言.
2. RLHF 的时候 需要diversity 效果才会好，但是一旦diversity的话，就不太可能在你专门某一个子任务上面表现的很细致的. 

3.所以, 我在人工智能研究院 在研究alignment 其实本质上 是让model在更深层次上去理解我们的语言. 
理解人类通过语言 表达的意图, 当这个意图变得复杂的时候，我们是怎么去研究的. 

我是真的 无语
结果这篇paper  Reject
